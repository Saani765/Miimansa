# Medical Named Entity Recognition and Linking

This project performs Named Entity Recognition (NER) on medical forum posts to identify mentions of Drugs, Diseases, Symptoms, and Adverse Drug Reactions (ADRs). It then attempts to link the extracted ADRs to SNOMED-CT medical codes.

## Dataset

The project uses the [CADEC (Corpus of Adverse Drug Events)](https://data.mendeley.com/datasets/b7z9zrx924/1) dataset. The dataset is expected to be in the `cadec/` directory with the following structure:
- `cadec/text/`: Raw text files of forum posts.
- `cadec/original/`: Annotations with labels for ADR, Drug, Disease, Symptom.
- `cadec/meddra/`: Annotations using the MedDRA terminology.
- `cadec/sct/`: Annotations linked to SNOMED-CT codes.

## Project Workflow

The project is divided into several steps:

### Step 1: Entity Enumeration
- **Script**: `step1_entity_enumeration.py`
- **Purpose**: This script provides an initial analysis of the dataset. It parses the `.ann` files from the `cadec/original` directory to count the number of unique entities for each category (ADR, Drug, Disease, Symptom). This helps in understanding the distribution and variety of entities in the corpus.

### Step 2: NER using a Pre-trained Language Model
- **Script**: `step2_llm_sequence_labelling.py` (and `step2.ipynb` for an interactive version)
- **Purpose**: This is the core NER step. It uses a pre-trained biomedical NER model from Hugging Face (`d4data/biomedical-ner-all`) to label sequences of text.
- **Process**:
    1. Reads a forum post from the `cadec/text` directory.
    2. Applies the NER pipeline to the text.
    3. Post-processes the results to merge sub-word tokens into complete entities.
    4. Maps the model's output labels to the four target categories: `ADR`, `Drug`, `Disease`, `Symptom`.
    5. Saves the predictions in a `.ann`-style format.

### Step 3: Standard Evaluation
- **Script**: `step3_evaluate_predictions.py`
- **Purpose**: This script evaluates the performance of the NER model from Step 2.
- **Method**: It compares the predicted entities against the ground truth annotations in `cadec/original`. The evaluation is based on strict matching: a prediction is only correct if both the entity text and the label exactly match the ground truth. It calculates standard metrics: Precision, Recall, and F1-score.

### Step 4: ADR-focused Evaluation with MedDRA
- **Script**: `step4.py`
- **Purpose**: This step provides a specialized evaluation focused solely on the `ADR` label.
- **Method**: It uses the `cadec/meddra` annotations as the ground truth, treating every annotated entity in them as an ADR. It then compares the model's `ADR` predictions against this ground truth to calculate Precision, Recall, and F1-score for this specific, important category.

### Step 5: Relaxed Evaluation
- **Script**: `step5_relaxed_eval.py`
- **Purpose**: To provide a more forgiving evaluation of the NER model. In NER, it's common for a model to correctly identify an entity but with slightly different start or end boundaries than the ground truth.
- **Method**: This script implements a "relaxed" evaluation metric. A prediction is considered a true positive if its span (start and end characters) overlaps with a ground truth span of the same label. This provides a more nuanced view of the model's performance.

### Step 6: Entity Linking to SNOMED-CT
- **Script**: `step6.py`
- **Purpose**: This is an advanced step that goes beyond NER to perform entity linking. It attempts to normalize the detected `ADR` entities by linking them to concepts in the SNOMED-CT medical terminology.
- **Method**: For each ADR entity text extracted from `cadec/original`, this script tries to find the best matching SNOMED-CT concept from the `cadec/sct` annotations using two different techniques:
    1.  **Fuzzy String Matching**: Using the `fuzzywuzzy` library to find textually similar phrases.
    2.  **Sentence Embeddings**: Using a `sentence-transformer` model to find semantically similar concepts based on vector representations of the text.

## How to Run

1.  Ensure you have Python 3 installed and the required packages (`transformers`, `torch`, `fuzzywuzzy`, `sentence-transformers`, etc.). You can typically install them using `pip`.
2.  Make sure the CADEC dataset is placed in the `cadec/` directory.
3.  Run the scripts sequentially, starting from `step1` or `step2`. The output of one step is often the input for the next. For example, `step2` generates `predicted.ann`, which can be used by `step3` and `step4`. The `_predicted_spans.json` files generated by `batch_generate_predicted_spans.py` are used by `step5`.

## File Descriptions
- `*.py`: Python scripts for each step of the pipeline.
- `*.ipynb`: Jupyter notebooks for interactive exploration of the steps.
- `cadec/`: Directory containing the CADEC dataset.
- `*_predicted_spans.json`: JSON files containing the predicted entity spans for each processed text file, used in later evaluation steps.
- `predicted.ann`: A single file containing predictions in `.ann` format for a sample run.
- `step5_sampled_files.txt`: A list of files used for the relaxed evaluation in Step 5. 
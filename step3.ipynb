{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import defaultdict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_ann_file(filepath):\n",
    "    \"\"\"\n",
    "    Reads a .ann file and parses entities into a list of tuples.\n",
    "    Each tuple contains: (label, start, end, text)\n",
    "    Args:\n",
    "        filepath (str): Path to the .ann file.\n",
    "    Returns:\n",
    "        list of tuples: (label, start, end, text)\n",
    "    \"\"\"\n",
    "    entities = []\n",
    "    with open(filepath, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            line = line.strip()\n",
    "            if not line or line.startswith('#'):\n",
    "                continue  # Skip comments and empty lines\n",
    "            parts = line.split('\\t')\n",
    "            if len(parts) < 3:\n",
    "                continue  # Skip malformed lines\n",
    "            # Example: T1\\tDrug 44 49\\tzocor\n",
    "            label_ranges = parts[1]\n",
    "            entity_text = parts[2]\n",
    "            label_parts = label_ranges.split(' ')\n",
    "            label = label_parts[0]\n",
    "            # Some entities may have multiple ranges (e.g., 44 49;50 55), but for now, handle single range\n",
    "            try:\n",
    "                start = int(label_parts[1])\n",
    "                end = int(label_parts[2])\n",
    "            except (IndexError, ValueError):\n",
    "                continue  # Skip if indices are not valid\n",
    "            entities.append((label, start, end, entity_text))\n",
    "    return entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_entities(pred_entities, gt_entities):\n",
    "    \"\"\"\n",
    "    Compares predicted and ground truth entities for exact matches.\n",
    "    Args:\n",
    "        pred_entities (list of tuples): Predicted entities (label, start, end, text)\n",
    "        gt_entities (list of tuples): Ground truth entities (label, start, end, text)\n",
    "    Returns:\n",
    "        set: True positives (matched entities)\n",
    "        set: False positives (predicted but not in ground truth)\n",
    "        set: False negatives (ground truth but not predicted)\n",
    "    \"\"\"\n",
    "    pred_set = set(pred_entities)\n",
    "    gt_set = set(gt_entities)\n",
    "    true_positives = pred_set & gt_set\n",
    "    false_positives = pred_set - gt_set\n",
    "    false_negatives = gt_set - pred_set\n",
    "    return true_positives, false_positives, false_negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(tp, fp, fn):\n",
    "    \"\"\"\n",
    "    Computes precision, recall, and F1-score.\n",
    "    Args:\n",
    "        tp (set): True positives\n",
    "        fp (set): False positives\n",
    "        fn (set): False negatives\n",
    "    Returns:\n",
    "        dict: Precision, recall, and F1-score\n",
    "    \"\"\"\n",
    "    precision = len(tp) / (len(tp) + len(fp)) if (len(tp) + len(fp)) > 0 else 0.0\n",
    "    recall = len(tp) / (len(tp) + len(fn)) if (len(tp) + len(fn)) > 0 else 0.0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "    return {\"precision\": precision, \"recall\": recall, \"f1\": f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Ground Truth Entities ---\n",
      "('ADR', 9, 19, 'bit drowsy')\n",
      "('ADR', 29, 50, 'little blurred vision')\n",
      "('Drug', 93, 102, 'Arthrotec')\n",
      "('Disease', 179, 188, 'arthritis')\n",
      "('Symptom', 260, 265, 'agony')\n",
      "('ADR', 62, 78, 'gastric problems')\n",
      "('Symptom', 412, 417, 'pains')\n",
      "('ADR', 437, 453, 'feel a bit weird')\n",
      "\n",
      "--- Predicted Entities ---\n",
      "('Symptom', 13, 19, 'drowsy')\n",
      "('Symptom', 36, 43, 'blurred')\n",
      "('ADR', 93, 96, 'Art')\n",
      "('Drug', 96, 102, 'hrotec')\n",
      "('Disease', 179, 188, 'arthritis')\n",
      "('Symptom', 412, 417, 'pains')\n",
      "('ADR', 460, 471, 't have that')\n",
      "\n",
      "--- Evaluation Metrics ---\n",
      "Precision: 0.29\n",
      "Recall:    0.25\n",
      "F1-score:  0.27\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Example: Evaluate predictions for a single file\n",
    "    # TODO: Replace with actual file paths and predicted results from step 2\n",
    "    pred_ann_file = \"predicted.ann\"  # Placeholder for predicted .ann file\n",
    "    gt_ann_file = os.path.join(\"cadec\", \"original\", \"ARTHROTEC.1.ann\")  # Example ground truth file\n",
    "    # Example: Read and print entities from a ground truth .ann file\n",
    "    print(\"--- Ground Truth Entities ---\")\n",
    "    gt_entities = read_ann_file(gt_ann_file)\n",
    "    for ent in gt_entities:\n",
    "        print(ent)\n",
    "    # Example: Read predicted entities (replace with actual file in practice)\n",
    "    print(\"\\n--- Predicted Entities ---\")\n",
    "    if os.path.exists(pred_ann_file):\n",
    "        pred_entities = read_ann_file(pred_ann_file)\n",
    "        for ent in pred_entities:\n",
    "            print(ent)\n",
    "    else:\n",
    "        pred_entities = []\n",
    "        print(\"No predicted.ann file found. Please generate predictions from step 2.\")\n",
    "    # Compare and compute metrics\n",
    "    tp, fp, fn = compare_entities(pred_entities, gt_entities)\n",
    "    metrics = compute_metrics(tp, fp, fn)\n",
    "    print(\"\\n--- Evaluation Metrics ---\")\n",
    "    print(f\"Precision: {metrics['precision']:.2f}\")\n",
    "    print(f\"Recall:    {metrics['recall']:.2f}\")\n",
    "    print(f\"F1-score:  {metrics['f1']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
